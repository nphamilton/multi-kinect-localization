\begin{abstract}
For a drone to fly from Point A to Point B in an Euclidean environment, it must know where Points A and B lie, as well as any obstacles around or between those points, plus the positions of other robots and mobile obstacles in the environment in relation to itself. A common way to determine these positions is for the robots to find their distances from other objects. With video and radar sensors, this localization and mapping process can be accomplished using robotmounted cameras, external cameras, or with mounted cameras and sensors, often culminating in a Simultaneous Localization and Mapping (SLAM) process. Self-determined localization using mounted sensors and cameras is expensive, complicated, and onerous, so in StarL, we use a more generalized approach with a distributed network of computers each connected to one Kinect camera. This single camera determines the location and orientation of each robot and then sends that information to each robot’s control system, as well as the estimates of obstacle locations. This method is more cost-effective and easy to set up because it only requires one camera for multiple robots instead of a set of cameras on or for each robot. However, the cost-effectiveness of using a single camera impedes progress with a limited field of view. Although, this is a similar disadvantage with multi-camera localization systems, such as Vicon or OptiTrack, which have a limited volume in which they may estimate positions. In past versions of StarL, when any robot moved out of the Kinect’s view, the entire system would stop working for that robot and it would begin to drift aimlessly. This poster expands the complete field of view by adding Kinects to a distributed, networked system, instantiated as x86 MinnowBoards, so that the position and identity information of any robot can be passed from one Kinect to another as robots pass from one field of view to the next, as shown in Figure 1. To improve processing, the MinnowBoards may operate as fog (local) computing nodes when processing the video data in the cloud. The result is a collection of Kinect cameras working together to provide localization information across a much larger area, and overall represents a sophisticated distributed cyber-physical system (CPS).

\end{abstract}

\begin{IEEEkeywords}
Cyber-Physical Systems

\end{IEEEkeywords}